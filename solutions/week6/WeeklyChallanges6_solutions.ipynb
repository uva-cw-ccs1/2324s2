{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "formal-tomorrow",
   "metadata": {},
   "source": [
    "# Challenges week 6\n",
    "\n",
    "Now that you have some experience with working with APIs and playing around with this data, it's time for you to combine and apply your knowledge. You will start working on these challenges in the tutorial and will be asked to complete them by the end of the week. \n",
    "\n",
    "In each challenge, you are asked to provide the programming solution to it as well as a technical interpretation explaining the steps taken and the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3860f173",
   "metadata": {},
   "source": [
    "# Challenge 1\n",
    "\n",
    "The list below contains fictional article headlines. You are asked to pre-process the headlines and provide some basic information about them.\n",
    "\n",
    "1. Pre-process the headlines (make sure to remove unnecessary characters and make the headline lower space)\n",
    "\n",
    "2. What is the average length of a headline?\n",
    "\n",
    "3. How many of the headlines are about cats or dogs?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7eb66dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = [\n",
    "    \"  Local man wins national pie-eating contest\",\n",
    "    \"New study shows that cats are good for you  \",\n",
    "    \"Breaking news: \\n Cat stuck in tree\",\n",
    "    \"Weather forecast: \\n Raining cats and dogs\",\n",
    "    \"Expert says that chocolate is the key to happiness \",\n",
    "    \"Exclusive interview with the world's oldest person\",\n",
    "    \" Local school wins state basketball championship\",\n",
    "    \"Police arrest suspect in bank robbery \",\n",
    "    \"  Dog bites man in park\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc72f514",
   "metadata": {},
   "source": [
    "My analysis plan:\n",
    "I will first loop over the list and create a new list with pre-processed headlines. In the loop, I will use `strip()` and `lower()`. Next, I will calculate the average by again looping over the list. Finally, I will use `in` to determine how many headlines contain the words cat, cats, dog or dogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce9dd9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here I wrote the loop that 1) removes spaces, 2) lowercases everything and 3) replaces \\n\n",
    "headlines_pre = []\n",
    "\n",
    "for headline in headlines:\n",
    "    headlines_pre.append(headline.strip().lower().replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fd66f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['local man wins national pie-eating contest', 'new study shows that cats are good for you', 'breaking news:  cat stuck in tree', 'weather forecast:  raining cats and dogs', 'expert says that chocolate is the key to happiness', \"exclusive interview with the world's oldest person\", 'local school wins state basketball championship', 'police arrest suspect in bank robbery', 'dog bites man in park']\n"
     ]
    }
   ],
   "source": [
    "#check if the new list looks ok\n",
    "print(headlines_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a617098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An average headline is 40.22 characters long\n"
     ]
    }
   ],
   "source": [
    "lenall = 0\n",
    "for headline in headlines_pre:\n",
    "    lenall += len(headline)\n",
    "\n",
    "print(f\"An average headline is {lenall/len(headlines_pre):.2f} characters long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37185936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I will start with creating a function that checks if cat or dog is in the headline.\n",
    "\n",
    "def dogcatcheck(text):\n",
    "    '''takes a string as input and checks if dog or cat is in the string '''\n",
    "    if (\"dog\" in text) or (\"cat\" in text):\n",
    "        return 1 #return 1 if one of the two conditions is met\n",
    "    else:\n",
    "        return 0 #return 0 in all other cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c107c056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 headlines about cats or dogs.\n"
     ]
    }
   ],
   "source": [
    "numdogcat = 0\n",
    "for headline in headlines_pre:\n",
    "    numdogcat+=dogcatcheck(headline)\n",
    "\n",
    "print(f\"There are {numdogcat} headlines about cats or dogs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-hands",
   "metadata": {},
   "source": [
    "# Challange 2\n",
    "\n",
    "Here is a list of easy to use APIs related to entertainment https://github.com/public-apis/public-apis#entertainment with little or no authentication. We will now use the RandomUselessFacts API, for which the documentation is very simple and can be found here https://uselessfacts.jsph.pl/. \n",
    "1. Interact with the API to get access to a random fact or the fact of the day. \n",
    "2. Write a function that builds upon your initial script and allows for increased functionality/interaction with the API. The most apparent option here is to write a function that allows you to collect many facts using the /random endpoint. For the function, provide the necessary code and a short explanation of the working of the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f4a2708-0e87-45aa-afdc-30f1ea13d8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "\n",
    "a_fact = requests.get('https://uselessfacts.jsph.pl/today.json')\n",
    "a_fact\n",
    "#I get the appropriate response code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16b10409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\"id\":\"9b780d6d5f6021b2aa1c8f2f62f6e2c9\",\"text\":\"400-quarter pounders can be made from 1 cow.\",\"source\":\"djtech.net\",\"source_url\":\"http://www.djtech.net/humor/useless_facts.htm\",\"language\":\"en\",\"permalink\":\"https://uselessfacts.jsph.pl/api/v2/facts/9b780d6d5f6021b2aa1c8f2f62f6e2c9\"}\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here's the content\n",
    "a_fact.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7daf63a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'{\"id\":\"7f69cfb718747173f86b1630b5369724\",\"text\":\"There is a town in Newfoundland, Canada called Dildo.\",\"source\":\"djtech.net\",\"source_url\":\"http://www.djtech.net/humor/useless_facts.htm\",\"language\":\"en\",\"permalink\":\"https://uselessfacts.jsph.pl/api/v2/facts/7f69cfb718747173f86b1630b5369724\"}\\n']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A function that retrieves thrity random facts\n",
    "fact_list = []\n",
    "def get_a_fact():\n",
    "    for i in range(1,30):\n",
    "        a_fact =requests.get('https://uselessfacts.jsph.pl/random.json?language=en')\n",
    "        fact_list.append(a_fact.content)\n",
    "        return fact_list\n",
    "my_facts = get_a_fact()\n",
    "my_facts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-campaign",
   "metadata": {},
   "source": [
    "# Challenge 3 \n",
    "\n",
    "The dataset `example_tweets.json` is the json  response returned from the Twitter API. This is a data collected using the keyword search surveillance - tweets that include this keyword are included in the dataset. On [this website](https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet) you can find the documentation of the API and the columns it returns. In this challenge, you will explore this data and identify possible privacy and ethical issues with it.\n",
    "\n",
    "\n",
    "1. Use pandas to load this data into a dataframe. \n",
    "\n",
    "2. Explore this dataset using the skills you have learned in the class. Make sure to check how the dataset looks like, what columns it has, what data type these columns have and if there are any missing values. Write a short description of the dataset and answer the following questions:\n",
    "* How many languages are the tweets in?\n",
    "* How many retweets did the tweets receive on average?\n",
    "* Is the retweets number higher or lower for tweets that contain sensitive links vs. the one's with non-sensitive and no links? *Tip: check in the documentation what missing values in column `possibly_sensitive` mean and consider what you need to do with them.*\n",
    "\n",
    "3. Imagine that you want to research if there is a relation between the topics in the text of the tweets about surveillance and number of retweets the tweets receive. You want to include number of followers of the author of each tweet as a control variable. Hence, you need the columns with information on the text of the tweet (`text`), its number of retweets (`retweet_count`) and the number of followers of the author (included in `user`). \n",
    "* Minimize the dataset accordingly. Explain your minimization steps.\n",
    "* What other steps would you need to take to make sure that the data follows privacy protection principles discussed in the lecture? Give concrete examples based on the information available in your minimized dataset. *Tip: consider what identifiable and identified information the text and user columns contain.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8351fc8",
   "metadata": {},
   "source": [
    "### Q1\n",
    "\n",
    "I will first load the data from the json file directly into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40f8c4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "import pandas as pd\n",
    "data = pd.read_json(\"example_tweets.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7ac33e",
   "metadata": {},
   "source": [
    "### Q2\n",
    "\n",
    "Next, I explore the data using the code we have learned in class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df8a603b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_at                   datetime64[ns]\n",
       "id                                    int64\n",
       "id_str                                int64\n",
       "full_text                            object\n",
       "truncated                              bool\n",
       "display_text_range                   object\n",
       "entities                             object\n",
       "metadata                             object\n",
       "source                               object\n",
       "in_reply_to_status_id               float64\n",
       "in_reply_to_status_id_str           float64\n",
       "in_reply_to_user_id                 float64\n",
       "in_reply_to_user_id_str             float64\n",
       "in_reply_to_screen_name              object\n",
       "user                                 object\n",
       "geo                                 float64\n",
       "coordinates                         float64\n",
       "place                                object\n",
       "contributors                        float64\n",
       "retweeted_status                     object\n",
       "is_quote_status                        bool\n",
       "quoted_status_id                    float64\n",
       "quoted_status_id_str                float64\n",
       "retweet_count                         int64\n",
       "favorite_count                        int64\n",
       "favorited                              bool\n",
       "retweeted                              bool\n",
       "lang                                 object\n",
       "quoted_status                        object\n",
       "possibly_sensitive                  float64\n",
       "extended_entities                    object\n",
       "withheld_in_countries                object\n",
       "dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0a971b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['created_at', 'id', 'id_str', 'full_text', 'truncated',\n",
       "       'display_text_range', 'entities', 'metadata', 'source',\n",
       "       'in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
       "       'in_reply_to_user_id', 'in_reply_to_user_id_str',\n",
       "       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',\n",
       "       'contributors', 'retweeted_status', 'is_quote_status',\n",
       "       'quoted_status_id', 'quoted_status_id_str', 'retweet_count',\n",
       "       'favorite_count', 'favorited', 'retweeted', 'lang', 'quoted_status',\n",
       "       'possibly_sensitive', 'extended_entities', 'withheld_in_countries'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "080962ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_at                      0\n",
       "id                              0\n",
       "id_str                          0\n",
       "full_text                       0\n",
       "truncated                       0\n",
       "display_text_range              0\n",
       "entities                        0\n",
       "metadata                        0\n",
       "source                          0\n",
       "in_reply_to_status_id         885\n",
       "in_reply_to_status_id_str     885\n",
       "in_reply_to_user_id           882\n",
       "in_reply_to_user_id_str       882\n",
       "in_reply_to_screen_name       882\n",
       "user                            0\n",
       "geo                          1000\n",
       "coordinates                  1000\n",
       "place                         994\n",
       "contributors                 1000\n",
       "retweeted_status              289\n",
       "is_quote_status                 0\n",
       "quoted_status_id              920\n",
       "quoted_status_id_str          920\n",
       "retweet_count                   0\n",
       "favorite_count                  0\n",
       "favorited                       0\n",
       "retweeted                       0\n",
       "lang                            0\n",
       "quoted_status                 983\n",
       "possibly_sensitive            777\n",
       "extended_entities             911\n",
       "withheld_in_countries         999\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73805fe",
   "metadata": {},
   "source": [
    "After examining the dataset, I can conclude that it has 1000 rows with 32 columns, including created_at, id, etc. Some columns have many missing values (800+). However, the columns we are interested in can be worked with and are of the expected variable type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9fa45f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en     832\n",
       "fr     109\n",
       "ca      25\n",
       "nl      11\n",
       "und      7\n",
       "es       3\n",
       "it       3\n",
       "tl       2\n",
       "in       2\n",
       "de       2\n",
       "ja       2\n",
       "hi       1\n",
       "th       1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many languages question \n",
    "data['lang'].value_counts()\n",
    "#English is the dominant language of the tweets, with tweets available in 13 languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "122ace43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On average, a tweet gets 417.352 retweets\n"
     ]
    }
   ],
   "source": [
    "#average number of retweets \n",
    "print(f\"On average, a tweet gets {data['retweet_count'].mean()} retweets\")\n",
    "#the mean rt count is 417,352"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbbe89d",
   "metadata": {},
   "source": [
    "I have checked the documentation and I can assume that a missing value in the column `possibly_sensitive` means that the tweet is not possibly sensitive (the authors did not fill this field in). I can hence fill in the missing values with 0. Note that this is an assumption!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d1d1034",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here I replace all missing values in the column possibly_sensitive with 0\n",
    "data['possibly_sensitive'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a52b7c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "possibly_sensitive\n",
       "0.0    420.091919\n",
       "1.0    146.100000\n",
       "Name: retweet_count, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To get separate means for possibly sensitive and not sensitive tweets I use groupby. \n",
    "data.groupby(['possibly_sensitive'])['retweet_count'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56fdc8b",
   "metadata": {},
   "source": [
    "### Q3\n",
    "\n",
    "For this question, I need to minimize the dataset. I only need three columns meaning I should remove all other columns not to process data that I do not need for my RQ. I do so by slicing the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88590e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @AP: BREAKING: Iran says it will allow UN t...</td>\n",
       "      <td>100</td>\n",
       "      <td>{'id': 3264870655, 'id_str': '3264870655', 'na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @emeriticus: Bush launched murderous wars f...</td>\n",
       "      <td>298</td>\n",
       "      <td>{'id': 1357150919192944640, 'id_str': '1357150...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @scroll_in: #PegasusRow \\n\\nCentre refuses ...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'id': 1124167148, 'id_str': '1124167148', 'na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@mararara44 @JudahWorldChamp @kprather88 @CDCg...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'id': 15497992, 'id_str': '15497992', 'name':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@CoinMarketCap financial surveillance</td>\n",
       "      <td>0</td>\n",
       "      <td>{'id': 177506855, 'id_str': '177506855', 'name...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_text  retweet_count  \\\n",
       "0  RT @AP: BREAKING: Iran says it will allow UN t...            100   \n",
       "1  RT @emeriticus: Bush launched murderous wars f...            298   \n",
       "2  RT @scroll_in: #PegasusRow \\n\\nCentre refuses ...              1   \n",
       "3  @mararara44 @JudahWorldChamp @kprather88 @CDCg...              0   \n",
       "4              @CoinMarketCap financial surveillance              0   \n",
       "\n",
       "                                                user  \n",
       "0  {'id': 3264870655, 'id_str': '3264870655', 'na...  \n",
       "1  {'id': 1357150919192944640, 'id_str': '1357150...  \n",
       "2  {'id': 1124167148, 'id_str': '1124167148', 'na...  \n",
       "3  {'id': 15497992, 'id_str': '15497992', 'name':...  \n",
       "4  {'id': 177506855, 'id_str': '177506855', 'name...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smaller_data = data[['full_text', 'retweet_count', 'user']]\n",
    "smaller_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f0ee7",
   "metadata": {},
   "source": [
    "If we were to share this data, we would have to be careful with the `user` and `full_text` columns since both can contain identifying information both for the poster and for other Twitter users which may or may not be in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87780bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
